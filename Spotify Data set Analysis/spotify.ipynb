{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Load in the Data\n",
    "data = pd.read_csv(\"./data/spotify_millsongdata.csv\")\n",
    "\n",
    "# Split test set\n",
    "data, data_test = train_test_split(data, test_size = .8)\n",
    "\n",
    "columns = ['artist', 'song', \"text\"]\n",
    "data = data.loc[:, ['artist', 'song',\"text\"]]\n",
    "\n",
    "# Preview the data before modifying it\n",
    "print(\"===== Data Before Processing ====\")\n",
    "print(data.shape,\"\\n\", data.head())\n",
    "data_copy = data.copy() # save a version of the unprocessed data\n",
    "\n",
    "# Divide the data into the response (artist) versus the words\n",
    "X_full, y_full = data.copy().drop(['song', 'artist'], axis=1), data[\"artist\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# EXPLORING THE DATA\n",
    "To explore the data, I processed it in different ways that would support my goal of predicting the artist based on the words in their song.\n",
    "Initially, the data just has each artist, the song title, and the song lyrics for that entire song. In order to process the data, I used a Bag-of-words, technique, to make new features for the counts of each word in the song.  \n",
    "\n",
    "\n",
    "### 1. **Visualizing word frequencies across all songs in the data set**\n",
    "To visualize word frequencies, I used two methods: a bar chart and a word cloud. The bar chart helps us easily see the most frequent words. This includes words like \"the\" and \"i\" which we would expect to be very frequent. The word cloud was just for fun, but it does give some visual intuition on the difference between different artists. \n",
    "\n",
    "\n",
    "### 2. **Visualizing word counts of all artists**\n",
    "I added a feature in the data of the word count for each song. \n",
    "I then plotted a bar plot of artists to the average number of words in their songs. I thought this feature would be interesting as some artists (ex: rap artists) might have more words on average than others. It could also be interesting to visualize the number of unique words used by the artist.\n",
    "\n",
    "### 3. **Narrowing the data in on a subset of artists**\n",
    "-  ** Visualizing word frequencies for all the songs made by the selected artists**\n",
    "-  ** Visualizing word frequencies for a single artist**\n",
    "- For both of the above visualizations, I used the same methods as in (1).\n",
    "\n",
    "\n",
    "### 4. **Determining what word counts to add as features**\n",
    "- **When to standardize?** - one question I delt with was when to standardize the data. Because my data has text in a condensed form, I would first need to select a subset of words to add as features containing their word count (this is due to there being over 20,000 unique words in all songs) before standardizig. One idea I had was to just use the top 1,000 words across all the songs. However, one concern I had was whether this would just prioritize words that are in general common in English and not words that would help differentiate between artists since I couldn't standardize before selecting this initial set of features.\n",
    "- **Initial features/word selection rationale:** Because my goal is to predict the artist given a particular song / lyrics of the song, I decided to just add all the words in the selected artists dataset and then standardize the data. \n",
    "- After standardizing each column, I'll later perform **dimension reduction** to reduce the overall size of the feature space of the data set. It makes sense to do dimension reduction as words are likely correlated (ex: words like \"i\" might be correlated with \"you\"), and also we have a very large p>>, so for computational reasons this is a good idea. \n",
    "\n",
    "### 5. **Visualizing the new feature distributions** (histogram of the Bag-Of-Words columns across all data) \n",
    "I also visualized the word distributions and word correlations to get more insight into the words themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Visualization Helper Functions\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from collections import Counter\n",
    "import operator\n",
    "def get_word_counts(data, artist = None):\n",
    "    \"\"\"\n",
    "    Gets the word counts for a given name in the data in a dictionary\n",
    "    \"\"\"\n",
    "    if artist == None:\n",
    "        artist_data = data\n",
    "    else:\n",
    "        artist_data = data[data_copy[\"artist\"] == artist]\n",
    "    word_counts = data.copy().text.str.split(expand=True).stack().value_counts().reset_index()\n",
    "    word_counts.columns = ['Word', 'Count']\n",
    "    \n",
    "    return dict(zip(word_counts['Word'].tolist(), word_counts['Count'].tolist()))\n",
    "\n",
    "def plot_artist_words(data, artist = None):\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2, figsize = (15,7))\n",
    "    word_dict = get_word_counts(data, artist)\n",
    "\n",
    "    #plot the word bar plot\n",
    "    top_word_counts = dict(sorted(word_dict.items(), key=operator.itemgetter(1), reverse=True)[:20])\n",
    "    ax1.bar(top_word_counts.keys(), top_word_counts.values())\n",
    "    ax1.set_title(artist)\n",
    "\n",
    "    #plot the word cloud\n",
    "    word_cloud = WordCloud(background_color = 'white', stopwords = STOPWORDS, max_words = 100).generate_from_frequencies(word_dict)\n",
    "    ax2.imshow(word_cloud)\n",
    "    ax2.set_title(artist)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Visualizing word frequencies across all songs in the data set\n",
    "plot_artist_words(data_copy)\n",
    "\n",
    "# 2. Visualizing word counts of all artists\n",
    "data[\"song_wc\"] = [len(lyrics) for lyrics in data['text'].tolist()] # word counts\n",
    "\n",
    "# group the word counts of each song by artist and average\n",
    "song_lengths_df = data.groupby(\"artist\")[\"song_wc\"].apply(list).reset_index(name = \"song_wcs\")\n",
    "\n",
    "song_lengths_df[\"avg_song_wc\"] = 0\n",
    "for i in range(len(song_lengths_df[\"song_wcs\"])):\n",
    "    song_wcs = song_lengths_df[\"song_wcs\"].iloc[[i]]\n",
    "    song_wcs = song_wcs.tolist()[0]\n",
    "    avg = sum(song_wcs)/ len(song_wcs)\n",
    "    song_lengths_df[\"avg_song_wc\"].iloc[[i]] = avg\n",
    "print(song_lengths_df.head())\n",
    "\n",
    "song_lengths_df.plot.hist(column = \"avg_song_wc\", bins = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the data for a selected subset of the artists\n",
    "artists = [\"Eminem\", \"Metallica\", \"Rihanna\", \"Taylor Swift\"]\n",
    "\n",
    "def subset_data(X_full, y_full, classes):\n",
    "    \"\"\"\n",
    "    take a subset of the data that corresponds to the values in the list, classes\n",
    "    \"\"\"\n",
    "    cols = X_full.columns\n",
    "    X_full = X_full.to_numpy()\n",
    "    y_full = list(y_full.to_numpy())\n",
    "\n",
    "    # copy over data entries corresponding to the correct author\n",
    "    X, y = [], []\n",
    "    for i in range(len(X_full)):\n",
    "        if y_full[i] in artists:\n",
    "            X.append(X_full[i])\n",
    "            y.append(y_full[i])\n",
    "    return pd.DataFrame(X, columns = cols), y\n",
    "\n",
    "# Get the artist data set\n",
    "X, y = subset_data(X_full, y_full, artists)\n",
    "data_artists = X.copy()\n",
    "data_artists[\"artist\"] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Artist specific visualizations\n",
    "# Visualizing word frequencies for all the songs made by the selected artists\n",
    "plot_artist_words(data_artists) \n",
    "\n",
    "# Visualizing word frequencies for a single artist (do for each selected artist)\n",
    "for artist in artists:\n",
    "    print(artist,\" : num songs = \", list(data[\"artist\"]).count(artist))\n",
    "    plot_artist_words(data, artist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4428, 2)\n",
      "4428\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['I', 'the', 'you', 'to', 'a', 'me', 'I'm', 'it', 'my', 'and'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [84], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 5. Standardize the data\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Plot the distributions of the top 100 words in a histogram\u001b[39;00m\n\u001b[1;32m     14\u001b[0m top_unstandardized_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(word_count_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWord\u001b[39m\u001b[38;5;124m'\u001b[39m])[:\u001b[38;5;241m10\u001b[39m]\n\u001b[0;32m---> 15\u001b[0m X\u001b[38;5;241m.\u001b[39mplot\u001b[38;5;241m.\u001b[39mhist(column \u001b[38;5;241m=\u001b[39m top_unstandardized_words, bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, xlim \u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m80\u001b[39m))\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Standardize the data\u001b[39;00m\n\u001b[1;32m     18\u001b[0m scalar \u001b[38;5;241m=\u001b[39m StandardScaler()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/plotting/_core.py:1346\u001b[0m, in \u001b[0;36mPlotAccessor.hist\u001b[0;34m(self, by, bins, **kwargs)\u001b[0m\n\u001b[1;32m   1286\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhist\u001b[39m(\u001b[39mself\u001b[39m, by\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, bins\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1287\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m \u001b[39m    Draw one histogram of the DataFrame's columns.\u001b[39;00m\n\u001b[1;32m   1289\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[39m        >>> ax = df.plot.hist(column=[\"age\"], by=\"gender\", figsize=(10, 8))\u001b[39;00m\n\u001b[1;32m   1345\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1346\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kind\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mhist\u001b[39;49m\u001b[39m\"\u001b[39;49m, by\u001b[39m=\u001b[39;49mby, bins\u001b[39m=\u001b[39;49mbins, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/plotting/_core.py:972\u001b[0m, in \u001b[0;36mPlotAccessor.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    969\u001b[0m             label_name \u001b[39m=\u001b[39m label_kw \u001b[39mor\u001b[39;00m data\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m    970\u001b[0m             data\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m label_name\n\u001b[0;32m--> 972\u001b[0m \u001b[39mreturn\u001b[39;00m plot_backend\u001b[39m.\u001b[39;49mplot(data, kind\u001b[39m=\u001b[39;49mkind, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/plotting/_matplotlib/__init__.py:71\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(data, kind, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m         kwargs[\u001b[39m\"\u001b[39m\u001b[39max\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(ax, \u001b[39m\"\u001b[39m\u001b[39mleft_ax\u001b[39m\u001b[39m\"\u001b[39m, ax)\n\u001b[1;32m     70\u001b[0m plot_obj \u001b[39m=\u001b[39m PLOT_CLASSES[kind](data, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m---> 71\u001b[0m plot_obj\u001b[39m.\u001b[39;49mgenerate()\n\u001b[1;32m     72\u001b[0m plot_obj\u001b[39m.\u001b[39mdraw()\n\u001b[1;32m     73\u001b[0m \u001b[39mreturn\u001b[39;00m plot_obj\u001b[39m.\u001b[39mresult\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/plotting/_matplotlib/core.py:327\u001b[0m, in \u001b[0;36mMPLPlot.generate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    326\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_args_adjust()\n\u001b[0;32m--> 327\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_plot_data()\n\u001b[1;32m    328\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setup_subplots()\n\u001b[1;32m    329\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_plot()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/plotting/_matplotlib/core.py:470\u001b[0m, in \u001b[0;36mMPLPlot._compute_plot_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_kind \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mhist\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mbox\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    469\u001b[0m     cols \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mby \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mby\n\u001b[0;32m--> 470\u001b[0m     data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mloc[:, cols]\n\u001b[1;32m    472\u001b[0m \u001b[39m# GH15079 reconstruct data if by is defined\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mby \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/core/indexing.py:961\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m    960\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_value(\u001b[39m*\u001b[39mkey, takeable\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_takeable)\n\u001b[0;32m--> 961\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_tuple(key)\n\u001b[1;32m    962\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    963\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m    964\u001b[0m     axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/core/indexing.py:1149\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multi_take_opportunity(tup):\n\u001b[1;32m   1147\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multi_take(tup)\n\u001b[0;32m-> 1149\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_tuple_same_dim(tup)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/core/indexing.py:827\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_tuple_same_dim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[39mif\u001b[39;00m com\u001b[39m.\u001b[39mis_null_slice(key):\n\u001b[1;32m    825\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m--> 827\u001b[0m retval \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(retval, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\u001b[39m.\u001b[39;49m_getitem_axis(key, axis\u001b[39m=\u001b[39;49mi)\n\u001b[1;32m    828\u001b[0m \u001b[39m# We should never have retval.ndim < self.ndim, as that should\u001b[39;00m\n\u001b[1;32m    829\u001b[0m \u001b[39m#  be handled by the _getitem_lowerdim call above.\u001b[39;00m\n\u001b[1;32m    830\u001b[0m \u001b[39massert\u001b[39;00m retval\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/core/indexing.py:1194\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(key, \u001b[39m\"\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m key\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1192\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot index with multidimensional key\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_iterable(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[1;32m   1196\u001b[0m \u001b[39m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1197\u001b[0m \u001b[39mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/core/indexing.py:1132\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1132\u001b[0m keyarr, indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_listlike_indexer(key, axis)\n\u001b[1;32m   1133\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1134\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_dups\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/core/indexing.py:1330\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1327\u001b[0m ax \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1328\u001b[0m axis_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1330\u001b[0m keyarr, indexer \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39;49m_get_indexer_strict(key, axis_name)\n\u001b[1;32m   1332\u001b[0m \u001b[39mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py:5796\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5793\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5794\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5796\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   5798\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   5799\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5800\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/core/indexes/base.py:5856\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5854\u001b[0m     \u001b[39mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   5855\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 5856\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   5858\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m   5859\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['I', 'the', 'you', 'to', 'a', 'me', 'I'm', 'it', 'my', 'and'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# 4. Determining what word counts to add as features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# get the unique words in the artist data in a dataframe corresponding to the word counts\n",
    "word_count_data = data_artists.copy().text.str.split(expand=True).stack().value_counts().reset_index()\n",
    "word_count_data.columns = ['Word', 'Count']\n",
    "print(word_count_data.shape)\n",
    "word_list  = word_count_data['Word'].tolist()\n",
    "unique_words = set(word_list)\n",
    "print(len(unique_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Standardize the data\n",
    "# Plot the distributions of the top 100 words in a histogram\n",
    "top_unstandardized_words = list(word_count_data['Word'])[:10]\n",
    "X.plot.hist(column = top_unstandardized_words, bins=100, alpha=0.5, xlim =(0,80))\n",
    "\n",
    "# Standardize the data\n",
    "scalar = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scalar.fit_transform(X), columns = X.columns)\n",
    "\n",
    "X_scaled.plot.hist(column = top_unstandardized_words[:10], bins=100, alpha=0.5, xlim =(0,5)) #re-plot the distribution of the top words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the text data and Standardize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Turn text lower case and remove punctuation\n",
    "data[\"text\"] = data[\"text\"].str.lower().str.replace('[^\\w\\s]','')\n",
    "data_copy = data.copy() # save a copy of the data in solid string form\n",
    "data[\"text\"] = data[\"text\"].str.strip().str.split() #remove unnecessary spaces and turn into a list of words\n",
    "\n",
    "# Create a new dataframe with word frequencies of all words in the dataframe\n",
    "word_count_data = data_copy.copy().text.str.split(expand=True).stack().value_counts().reset_index()\n",
    "word_count_data.columns = ['Word', 'Count']\n",
    "\n",
    "# Visualize the distributions of words accross authors / standardize data\n",
    "top_unstandardized_words = list(word_count_data['Word'])[:100] # determine the top 1000 word counts as features\n",
    "X.plot.hist(column = top_unstandardized_words[:10], bins=100, alpha=0.5, xlim =(0,80)) #plot the distribution of the top words\n",
    "\n",
    "\n",
    "# Standardize the data\n",
    "scalar = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scalar.fit_transform(X), columns = X.columns)\n",
    "\n",
    "X_scaled.plot.hist(column = top_unstandardized_words[:10], bins=100, alpha=0.5, xlim =(0,5)) #re-plot the distribution of the top words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the Song Lyrics\n",
    "# Turn text lower case and remove punctuation\n",
    "data[\"text\"] = data[\"text\"].str.lower().str.replace('[^\\w\\s]','')\n",
    "data_copy = data.copy() # save a copy of the data in solid string form\n",
    "data[\"text\"] = data[\"text\"].str.strip().str.split() #remove unnecessary spaces and turn into a list of words\n",
    "\n",
    "\n",
    "# Add Features to the data\n",
    "# top_words = list(word_count_data['Word'])[:100] # determine the top 1000 word counts as features\n",
    "data[\"total_words\"] = [len(lyrics) for lyrics in data['text'].tolist()] # word counts\n",
    "for word in top_words:\n",
    "    data[word] =  data[\"text\"].map(lambda x: x.count(word))\n",
    "print(\"\\n\\n===== Data With Word Counts ====\\n\", data.head(), \"\\n data shape:\", data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Standardize the Data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Visualize the distributions of words accross authors / standardize data\n",
    "X.plot.hist(column = top_words[:10], bins=100, alpha=0.5, xlim =(0,80))\n",
    "\n",
    "# Standardize the data\n",
    "scalar = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scalar.fit_transform(X), columns = X.columns)\n",
    "\n",
    "X_scaled.plot.hist(column = top_words[:10], bins=100, alpha=0.5, xlim =(0,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from collections import Counter\n",
    "import operator\n",
    "\n",
    "def get_word_counts(artist = None):\n",
    "    \"\"\"\n",
    "    Gets the word counts for a given name in the data in a dictionary\n",
    "    \"\"\"\n",
    "    if artist == None:\n",
    "        artist_data = data_copy\n",
    "    else:\n",
    "        artist_data = data_copy[data_copy[\"artist\"] == artist]\n",
    "    word_counts = artist_data.copy().text.str.split(expand=True).stack().value_counts().reset_index()\n",
    "    word_counts.columns = ['Word', 'Count']\n",
    "    \n",
    "    return dict(zip(word_counts['Word'].tolist(), word_counts['Count'].tolist()))\n",
    "\n",
    "def plot_artist_words(artist = None):\n",
    "    fig, (ax1,ax2) = plt.subplots(1,2, figsize = (15,7))\n",
    "    word_dict = get_word_counts(artist)\n",
    "\n",
    "    #plot the word bar plot\n",
    "    top_word_counts = dict(sorted(word_dict.items(), key=operator.itemgetter(1), reverse=True)[:20])\n",
    "    ax1.bar(top_word_counts.keys(), top_word_counts.values())\n",
    "    ax1.set_title(artist)\n",
    "\n",
    "    #plot the word cloud\n",
    "    word_cloud = WordCloud(background_color = 'white', stopwords = STOPWORDS, max_words = 100).generate_from_frequencies(word_dict)\n",
    "    ax2.imshow(word_cloud)\n",
    "    ax2.set_title(artist)\n",
    "    plt.show()\n",
    "\n",
    "# Create a word cloud and bar chart of all of the words in the songs\n",
    "plot_artist_words()\n",
    "\n",
    "# # Sort the artists by the number of songs in the database\n",
    "# artists = list(set(data[\"artist\"])) # unique artist list\n",
    "# artists_sorted = sorted(artists, key = lambda x: list(data[\"artist\"]).count(x), reverse = True)\n",
    "\n",
    "# # Preview the word counts of artists ranging in \"popularity\"\n",
    "for artist in artists:\n",
    "    print(artist,\" : num songs = \", list(data[\"artist\"]).count(artist))\n",
    "    plot_artist_words(artist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# ---------------- Helper Functions----------------\n",
    "\n",
    "def get_color_array(labels, y):\n",
    "    \"\"\" \n",
    "    Get a color array corresponding a color to each label\n",
    "    \"\"\"\n",
    "    label_colors = {}\n",
    "    cmap = plt.get_cmap('Spectral')\n",
    "    colormap = cmap(np.linspace(0, 1, len(labels))) #colors for each label\n",
    "    for i, label in enumerate(labels):\n",
    "        label_colors[label] = colormap[i]\n",
    "    color_array = [label_colors[label] for label in y]\n",
    "    return color_array, label_colors\n",
    "\n",
    "# ---------------- More Data Processing ----------------\n",
    "\n",
    "# 2. Map artists to colors for plotting purposes\n",
    "color_array, artist_colors = get_color_array(artists, y)\n",
    "\n",
    "# ---------------- PCA Helper Functions----------------\n",
    "def viz_2Dembedding(X_embedded, model_name):\n",
    "    plt.scatter(X_embedded[:,0], X_embedded[:,1], c = color_array)\n",
    "    custom_points = [Line2D([0], [0] , color = artist_colors[artist] , lw = 4) for artist in artists]\n",
    "    plt.legend(custom_points, artists)\n",
    "    \n",
    "    plt.xlabel(\"component 0\")\n",
    "    plt.ylabel(\"component 1\")\n",
    "    plt.title(\"Artist Words\")\n",
    "\n",
    "    plt.title(model_name + \" Visualization\")\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "def viz_feature_weights(features, feature_weights, model_name):\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,8))\n",
    "    ax1.bar(features, feature_weights[0][:len(features)])\n",
    "    ax1.set_title(\"feature 1 weights\")\n",
    "    ax1.set_ylabel(\"feature weights\")\n",
    "    ax1.set_xlabel(\"words\")\n",
    "\n",
    "    ax2.bar(features, feature_weights[1][:len(features)])\n",
    "    ax2.set_title(\"feature 2 weights\")\n",
    "    ax2.set_ylabel(\"feature weights\")\n",
    "    ax2.set_xlabel(\"words\")\n",
    "    plt.suptitle(model_name + \" Features\")\n",
    "    plt.show()\n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNSUPERVISED LEARNING / EXPLORING THE DATA\n",
    "words_to_viz = top_words[:20]\n",
    "\n",
    "# PCA Embedding\n",
    "model = PCA(n_components = 2)\n",
    "X_embedded = model.fit_transform(X)\n",
    "viz_2Dembedding(X_embedded, \"PCA\")\n",
    "viz_feature_weights(words_to_viz, model.components_, \"PCA\")\n",
    "\n",
    "# # NMF Embedding\n",
    "# model = NMF(n_components = 2)\n",
    "# X_embedded = model.fit_transform(X)\n",
    "# viz_2Dembedding(X_embedded, \"NMF\")\n",
    "# viz_feature_weights(words_to_viz, model.components_, \"NMF\")\n",
    "\n",
    "#tSNE Embedding\n",
    "model = TSNE(n_components = 2)\n",
    "X_embedded = model.fit_transform(X)\n",
    "viz_2Dembedding(X_embedded, \"tSNE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
